---
title: "Fitting count data"
author: "Jason Willwerscheid"
date: "2/10/2019"
output:
  workflowr::wflow_html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

I compare four methods for fitting a FLASH model to count data. I use three commonly-used transforms, the log1p transform
$$ Y_{ij} = \log \left( X_{ij} + 1 \right), $$
the Anscombe transform
$$ Y_{ij} = 2 \sqrt{ X_{ij} + \frac{3}{8}}, $$
and the square-root transform
$$ Y_{ij} = 2 \sqrt{X_{ij}}. $$

In the log1p case, I try adding mean factors to account for row- and column-specific scaling. Specifically, I fit a fixed row vector of all ones with column loadings $c_j$ to be estimated and a fixed column vector of all ones with row loadings $r_i$ to be estimated. This is approximately equivalent to estimating separate scaling factors for the rows and columns of the count data:
$$ X_{ij} + 1 = e^{r_i}e^{c_j} $$
Compare to the case where FLASH estimates a single rank-one factor with row loadings $r_i$ and column loadings $c_j$:
$$ X_{ij} + 1 = e^{r_i c_j} $$
Here, the scaling factors are not independent. Do note, however, that when $r_i$ and $c_j$ are both small,
$$ X_{ij} = e^{r_i c_j} - 1 \approx r_i c_j, $$
so fitting a single factor might work better when there is not a lot of baseline noise.

In the Anscombe case, fitting a single rank-one factor makes much more sense:
$$ X_{ij} + \frac{3}{8} = \frac{1}{4} r_i^2 c_j^2$$
Here, the usual rank-one FLASH model approximately estimates row- and column-specific scaling factors, whereas a model that includes fixed vectors of all ones introduces unwanted terms:
$$ X_{ij} + \frac{3}{8} = r_i c_j + \frac{r_i^2}{2} + \frac{c_j^2}{2} $$

Finally, in the square-root case, the rank-one model fits exact (rather than approximate) scaling factors:
$$ X_{ij} = \frac{1}{4} r_i^2 c_j^2. $$

## Simulating data

I simulate "null" models where the data is generated by baseline Poisson noise that has been separately scaled by row and column:
$$ X_{ij} \sim \text{Poisson}(r_i c_j \lambda) $$
To approximate scRNA data, I want the rows (cells) to be broadly similar, but I allow the column (gene) scaling factors to be very different. I draw
$$ r_i \sim \text{Unif}[0.5, 1.5],\ c_j \sim C t_2, $$
where $t_2$ is the half-$t$ distribution with 2 degrees of freedom and $C = \sqrt{2}$ is chosen such that $\mathbb{E}c_j = 1$. To encourage sparsity, I draw $\lambda$ from a Gamma distribution with shape parameter equal to 1 and rate parameter equal to 2.

```{r null_sim}
sim.null <- function(n, p, seed = 666) {
  set.seed(seed)
  lambda <- rgamma(1, shape = 1, rate = 2)
  row.scale <- runif(n, min = 0.5, max = 1.5)
  col.scale <- abs(rt(p, df = 2)) / sqrt(2)
  lambda.mat <- lambda * outer(row.scale, col.scale)
  return(list(data = matrix(rpois(n * p, lambda.mat), nrow = n, ncol = p),
              lambda = lambda))
}
```

## Experiments

To each simulated dataset, I fit eight FLASH objects: six using a log1p transform (with or without fixed ones vectors and using any of the three variance structures described in the code below) and one each using an Anscombe transform and a square root transform. The best that one could hope for is that the mean factors account for all of the structure in the transformed data. If so, then we might be able to hope that FLASH will give a good estimate of the "correct" number of factors when the data comes from a more complicated model.

```{r one_exp}
one.exp <- function(n, p, seed = 666, verbose = TRUE) {
  sim    <- sim.null(n, p, seed)
  X      <- sim$data
  lambda <- sim$lambda
  
  # Drop all-zero columns.
  X <- X[, colSums(X) > 0]
  p <- ncol(X)
  
  # Data transformations.
  logY  <- log1p(X)
  ansY  <- 2 * sqrt(X + 0.375)
  sqrtY <- 2 * sqrt(X)
  
  # When Z is distributed Poisson(lambda), the variance of log(Z + 1) is
  #   approximately lambda / (lambda + 1)^2. I test three ways of dealing
  #   with this heteroskedacity: 1. Set S^2 = X / (X + 1)^2. Add a 
  #   pseudo-count to zero counts so that the minimum S is 1 / 4. 2. Set
  #   S^2 = X / (X + 1)^2 and allow additional "noisy" variance to be
  #   estimated. 3. Estimate lambda using ash (take posterior means) and set
  #   S^2 = lambda.hat / (lambda.hat + 1)^2.
  S.X <- sqrt(X / (X + 1)^2)
  S.X.nozero <- S.X
  S.X.nozero[S.X.nozero == 0] <- 0.5
  ash.res <- ash(betahat = rep(0, n * p), sebetahat = 1,
                 lik = lik_pois(as.vector(X)), mode = 0)
  lambda.hat <- ash.res$result$PosteriorMean
  S.ash <- matrix(sqrt(lambda.hat / (lambda.hat + 1)^2), n, p)

  # Flashier parameters.
  fl.param     <- list(prior.type = "normal.mix",
                       ash.param = list(control = list(maxiter.sqp = 50000)), 
                       verbose = 2L * verbose)
  log.param1   <- list(data = logY, S = S.X.nozero, var.type = NULL)
  log.param2   <- list(data = logY, S = S.X, var.type = 0)
  log.param3   <- list(data = logY, S = S.ash, var.type = NULL)
  ans.param    <- list(data = ansY, S = 1, var.type = NULL)
  sqrt.param   <- list(data = sqrtY, S = 1, var.type = NULL)
  nomean.param <- list(greedy.Kmax = 10, backfit = "none")
  mean.param   <- list(greedy.Kmax = 9,
                       fix.dim = list(1, 2), fix.idx = list(1:n, 1:p),
                       fix.vals = list(rep(1, n), rep(1, p)),
                       backfit.after = 2, backfit.maxiter = 500,
                       final.backfit = FALSE)
  
  res <- list()
  res[[1]] <- do.call(flashier, c(fl.param, log.param1, nomean.param))
  res[[2]] <- do.call(flashier, c(fl.param, log.param2, nomean.param))
  res[[3]] <- do.call(flashier, c(fl.param, log.param3, nomean.param))
  res[[4]] <- do.call(flashier, c(fl.param, log.param1, mean.param))
  res[[5]] <- do.call(flashier, c(fl.param, log.param2, mean.param))
  res[[6]] <- do.call(flashier, c(fl.param, log.param3, mean.param))
  res[[7]] <- do.call(flashier, c(fl.param, ans.param, nomean.param))
  res[[8]] <- do.call(flashier, c(fl.param, sqrt.param, nomean.param))
  
  n.phantom <- lapply(res, function(fl) {
    pve <- fl$pve
    pve <- pve[pve > 0]
    if (length(fl$fit$fix.dim) > 0)
      n.phantom <- length(pve) - 2
    else 
      n.phantom <- length(pve) - 1
    return(n.phantom)
  })
  
  return(list(lambda = lambda,
              n.phantom = n.phantom))
}

many.exp <- function(n, p, seeds, verbose = TRUE) {
  res <- list()
  for (seed in seeds) {
    if (verbose)
      cat("SEED: ", seed, "\n")
    res <- c(res, list(one.exp(n, p, seed, verbose)))
  }
  return(res)
}
```

Because the experiments take a long time to run, I pre-run them and load the results from file.

```{r run_exp}
# devtools::load_all("~/Github/ashr")
# devtools::load_all("~/Github/flashier")
#  
# n <- 100
# p <- 200
# res <- many.exp(n, p, seeds = 1:25)
# saveRDS(res, "../data/count_preproc_r1/res.rds")

res <- readRDS("./data/count_preproc_r1/res.rds")
```

## Results

For each fit, I count the number of factors added. Ideally, the methods that don't use fixed ones vectors will add a single mean factor, while the ones that do use fixed vectors will not add any additional factors. I count any extra factors as "phantom" factors. In the plots below, a value of -1 indicates that the method did not add any factors at all (not even a mean factor).

```{r bars}
get.res.df <- function(res) {
  all.lambda <- sapply(res, `[[`, "lambda")
  
  method.names <- c("log1p method 1 (no fixed)",
                    "log1p method 2 (no fixed)",
                    "log1p method 3 (no fixed)",
                    "log1p method 1 (with fixed)",
                    "log1p method 2 (with fixed)",
                    "log1p method 3 (with fixed)",
                    "Anscombe transform",
                    "square root transform")
  n.methods <- length(method.names)
  
  n.phantom <- unlist(lapply(res, `[[`, "n.phantom"))
  return(data.frame(lambda = rep(all.lambda, each = n.methods),
                    method = as.factor(rep(method.names, length(all.lambda))),
                    n.phantom = n.phantom))
}
resdf <- get.res.df(res)

library(ggplot2)
ggplot(resdf, aes(x = n.phantom)) + 
  geom_bar(aes(fill = (n.phantom == 0), group = n.phantom)) + 
  facet_wrap(~ method) +
  xlab("Number of phantom factors") +
  scale_fill_manual(values = c("red", "green")) +
  guides(fill = FALSE)
```

The Anscombe transform correctly adds a single factor every time. The second best method, log1p method 3 (which uses `ashr` to fix the standard errors of the log-transformed data), only does well when fixed ones vectors are not added. The method that uses a simpler approach to fixing the standard errors (log1p method 1) does reasonably well with or without fixed ones vectors. Finally, the method that estimates the standard errors of the log-transformed data (log1p method 2) and the square-root transform both do very poorly. 

The behavior of each method depends to some extend on the value of $\lambda$. For example, log1p method 1 fares better with fixed ones vectors when $\lambda$ is very small, but better without when $\lambda$ is closer to 1. Method 3 appears to only make mistakes when $\lambda$ is very small.

```{r loess}
ggplot(resdf, aes(x = log10(lambda), y = n.phantom)) + 
  geom_smooth(aes(color = method), method = 'loess', formula = y ~ x) +
  ylab("Number of phantom factors")
```

For reference, the full results are as follows. To reduce clutter, I exclude all points where the correct number of factors are chosen.

```{r full}
ggplot(subset(resdf, n.phantom != 0), 
       aes(x = log10(lambda), y = n.phantom)) + 
  geom_jitter(aes(color = method)) +
  ylab("Number of phantom factors")
```
