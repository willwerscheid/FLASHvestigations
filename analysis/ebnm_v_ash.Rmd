---
title: "EBNM v ASH"
author: "Jason Willwerscheid"
date: "5/29/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Introduction

When the family of priors $G$ in the EBNM problem is a mixture of a point mass at zero and a single component with distribution $N(0, 1/a)$ (with $a$ fixed), then either `ebnm` or `ashr` can be used to estimate the mixture proportions $\pi_0$ and $1 - \pi_0$. I do a brief experiment comparing performance. As it turns out, `ebnm` is typically about an order of magnitude faster than `ashr`.

## Constant SEs

```{r const}
devtools::load_all("~/Github/ashr")
devtools::load_all("~/Github/ebnm")

pi0 <- 0.3
a <- 0.25
g <- ashr::normalmix(c(0.5, 0.5), c(0, 0), c(0, 1 / sqrt(a)))
ns <- c(100, 1000, 10000, 100000)
vtimes <- c(100, 100, 50, 10)
for (i in 1:length(ns)) {
  n <- ns[i]
  x <- c(rep(0, n * pi0), rnorm(n * (1 - pi0), mean = 0, sd = 1 / sqrt(a))) + rnorm(n)
  s <- 1
  bench <- microbenchmark::microbenchmark(ebnm(x, s, g = list(a = a), fix_a = TRUE),
                                          ash(x, s, g = g, prior = "uniform",
                                              outputlevel = "flash_data"),
                                          times = vtimes[i])
  nlab <- paste0("(n = ", n, ")")
  levels(bench$expr) <- paste(c("ebnm", "ashr"), nlab)
  plot(ggplot2::autoplot(bench))
}
```

## Vector of SEs

```{r se.vec}
for (i in 1:length(ns)) {
  n <- ns[i]
  s <- rgamma(n, 1, 1)
  x <- c(rep(0, n * pi0), rnorm(n * (1 - pi0), mean = 0, sd = 1 / sqrt(a))) + s * rnorm(n)
  bench <- microbenchmark::microbenchmark(ebnm(x, s, g = list(a = a), fix_a = TRUE),
                                          ash(x, s, g = g, prior = "uniform",
                                              outputlevel = "flash_data"),
                                          times = vtimes[i])
  nlab <- paste0("(n = ", n, ")")
  levels(bench$expr) <- paste(c("ebnm", "ashr"), nlab)
  plot(ggplot2::autoplot(bench))
}
```
