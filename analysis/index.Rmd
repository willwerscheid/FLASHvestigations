---
title: "Home"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: false
---

## In Progress

Investigations 8 and 9 implement parallel backfitting updates.

* [Investigation 8.](parallel.html) Parallelizing the backfitting algorithm shows promise.

* [Investigation 9.](parallel2.html) An additional trick is needed to parallelize the backfitting updates performed in this [MASH v FLASH GTEx analysis](https://willwerscheid.github.io/MASHvFLASH/MASHvFLASHgtex3.html).

[Investigation 10.](squarem.html) SQUAREM does poorly on FLASH backfits. DAAREM (a more recent algorithm by one of the authors of SQUAREM) does better, but offers smaller performance gains than parallelization.

[Investigation 11.](random.html) The order in which factor/loading pairs are updated (during backfitting) makes some difference, but not much.

[Investigation 12.](arbitraryV.html) To fit a FLASH model with an arbitrary error covariance matrix, I follow up on a [suggestion](https://github.com/stephenslab/flashr/issues/17) by Matthew Stephens.

Investigations 14 and 16-17 illustrate three approaches to factorizing the GTEx donation matrix. The first is more naive, and is primarily intended as an illustration of how to do nonnegative matrix factorization using FLASH. The second and third are more sophisticated approaches that model the entries as count or binary data.

* [Investigation 14.](nonnegative.html) An example of how to use nonnegative ASH priors to obtain a nonnegative matrix factorization.

* [Investigation 16.](count_data.html) Instead of directly fitting FLASH, I fit count data via a Gaussian approximation to the Poisson log likelihood...

* [Investigation 17.](binary_data.html) ... then I fit binary data via an approximation to the binomial log likelihood.

Note 3 and Investigation 18 explore stochastic approaches to fitting FLASH objects to very large datasets.

* [Note 3.](large_p.html) An idea for how to fit FLASH models when $n$ is manageable and $p$ is very large.

* [Investigation 18.](minibatch.html) I implement the idea described in Note 3 and I test it out on data from the GTEx project.

[Investigation 19.](trachea.html) I test out FLASH on a large single-cell RNA dataset.

[Note 3.](matrix_ops.html) I have been experiencing some performance issues when using FLASH to fit large datasets. Here I outline several ideas for reducing the memory footprint and improving overall performance. 

## Still Relevant

Notes 1 and 2 and Investigation 4 describe a way to compute the FLASH objective directly (rather than using the indirect method implemented in `flashr`).

* [Note 1.](obj_notes.html) Notes on computing the FLASH objective function. I derive an explicit expression for the KL divergence between prior and posterior. 

* [Note 2.](flash_em.html) An alternate algorithm for optimizing the FLASH objective, using the explicit expression derived in the previous note.

* [Investigation 4.](alt_alg.html) The alternate algorithm agrees with FLASH with respect to both the objective and fit obtained.

Investigations 5a-b and 13 attempt to determine the best default initialization function.

* [Investigation 5a.](init_fn.html) An argument for changing the default `init_fn` to `udv_si_svd` when there is missing data and `udv_svd` otherwise. Based on an analysis of GTEx data.

* [Investigation 5b.](init_fn2.html) More evidence supporting the recommendations in Investigation 5a. 

* [Investigation 13.](init_fn3.html) A counterargument. Results in Investigations 5a-b probably depend on the fact that $n$ is small ($n = 44$). For large $n$, setting `init_fn` to `udv_si` is best.


## Archived

The bug causing the problem described in Investigations 1-3 was fixed in version 0.1-13 of package `ebnm`.

* [Investigation 1.](objective.html) The FLASH objective function can behave very erratically.

* [Investigation 2.](objective2.html) This problem only occurs when using `ebnm_pn`, not `ebnm_ash`.

* [Investigation 3.](objective3.html) The objective can continue to get worse as loadings are repeatedly updated. Nonetheless, convergence takes place (from above!).

Investigations 6 and 7 deal with warmstarts, which were implemented in version 0.5-14 of `flashr`.

* [Investigation 6.](warmstart.html) Poor `optim` results can produce large decreases in the objective function. We should use warmstarts when `ebnm_fn = ebnm_pn`.

* [Investigation 7.](warmstart2.html) The advantages of warmstarts are not nearly as compelling when `ebnm_fn = ebnm_ash`.

The changes tested in Investigation 15 were implemented in version 0.6-2 of `flashr`.

* [Investigation 15.](scalar_tau.html) Tests an implementation of changes to the way `tau` is stored, as discussed  [here](https://github.com/stephenslab/flashr/issues/83).
