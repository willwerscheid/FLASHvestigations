---
title: "NPMLE via ebnm (equally spaced components)"
author: "Jason Willwerscheid"
date: "4/29/2020"
output:
  workflowr::wflow_html:
    code_folding: show
    toc: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE, comment = "#>", warning = TRUE)
```

I want to test out approximations of NPMLEs using a dense `ashr` grid. Let $x_1, \ldots, x_n$ be $n$ observations with standard errors equal to 1. [Dicker and Zhao](https://arxiv.org/abs/1407.2635) show that when the true NPMLE has compact support, then a good approximation can be obtained by optimizing over the family of distributions that's supported on $\sqrt{n}$ equally spaced points between $\min (x)$ and $\max (x)$. Instead of using point masses, I use an `ashr` grid with uniform components of equal width. Let's see how it works in practice.

Here's the true distribution which I'll be sampling from. It's bimodal with peaks at -5 and 5, so a unimodal prior family wouldn't work very well.

```{r true_g}
suppressMessages(library(tidyverse))

true_g <- ashr::normalmix(pi = rep(0.1, 10),
                          mean = c(rep(-5, 5), rep(5, 5)),
                          sd = c(0:4, 0:4))
cdf_grid <- seq(-20, 20, by = 0.1)
true_cdf <- drop(ashr::mixcdf(true_g, cdf_grid))
ggplot(tibble(x = cdf_grid, y = true_cdf), aes(x = x, y = y)) + geom_line()
```

I start by sampling 1000 observations and adding normally distributed noise.

```{r samples}
samp_from_g <- function(g, n) {
  comp <- sample(1:length(g$pi), n, replace = TRUE, prob = g$pi)
  mean <- g$mean[comp]
  sd <- g$sd[comp]
  return(rnorm(n, mean = mean, sd = sd))
}

set.seed(666)
n <- 1000
samp <- samp_from_g(true_g, n) + rnorm(n)
ggplot(tibble(x = samp), aes(x = x)) + geom_histogram(binwidth = 1)
```

I want to see how grid density affects convergence properties and the quality of the solution. From a log likelihood perspective, using a grid of points spaced at a distance that's about half the standard deviation of the noise gives a solution that's pretty much just as good as a very fine grid. Note that the Dicker and Zhao recommendation --- $\sqrt{n} \approx 32$, which corresponds to a `scale` of a little more than 0.8 --- produces a noticeably worse solution. 

```{r llik1000}
mixsqp_control = list()

scale_vec <- (max(samp) - min(samp) + 1e-4) / 2^(5:9)
res_list <- list()
for (scale in scale_vec) {
  ebnm_res <- ebnm::ebnm_npmle(samp, scale = scale, control = mixsqp_control)
  res_list <- c(res_list, list(ebnm_res))
}

ggplot(tibble(scale = scale_vec, 
              llik = sapply(res_list, `[[`, "log_likelihood")), 
       aes(x = scale, y = llik)) + 
  geom_point()
```

Visually, the CDFs are very similar for all `scale`s less than 0.5 SD:

```{r cdf1000}
cdf_df <- tibble(x = rep(cdf_grid, length(res_list)), 
                 y = as.vector(sapply(res_list, 
                                      function(res) drop(ashr::mixcdf(res$fitted_g, cdf_grid)))),
                 scale = as.factor(rep(round(scale_vec, 2), each = length(cdf_grid))))
ggplot(cdf_df, aes(x = x, y = y, col = scale)) + 
  geom_line()
```

Interestingly, the number of nonzero components is pretty much constant even as the total number of components increases:

```{r ncomp}
cat("Number of components:\n",
    sapply(res_list, function(res) length(res$fitted_g$pi)), "\n",
    "Number of nonzero components:\n",
    sapply(res_list, function(res) sum(res$fitted_g$pi > 0)))
```

I redo with 10000 observations. The same conclusions still hold, more or less. That the likelihood appears to decrease when large numbers of components are used is, I think, due to numerical error, but it says something about how accurate of a solution we should be aiming for. A good rule of thumb might be to set `scale` equal to $\text{SD} / (\log_{10} (n) - 1)$ (with a maximum of, say, 300 components).

```{r llik10000}
n <- 10000
samp <- samp_from_g(true_g, n) + rnorm(n)
scale_vec <- (max(samp) - min(samp) + 1e-4) / 2^(5:9)

res_list <- list()
for (scale in scale_vec) {
  ebnm_res <- ebnm::ebnm_npmle(samp, scale = scale, control = mixsqp_control)
  res_list <- c(res_list, list(ebnm_res))
}

ggplot(tibble(scale = scale_vec, 
              llik = sapply(res_list, `[[`, "log_likelihood")), 
       aes(x = scale, y = llik)) + 
  geom_point()

cdf_df <- tibble(x = rep(cdf_grid, length(res_list)), 
                 y = as.vector(sapply(res_list, 
                                      function(res) drop(ashr::mixcdf(res$fitted_g, cdf_grid)))),
                 scale = as.factor(rep(round(scale_vec, 2), each = length(cdf_grid))))
ggplot(cdf_df, aes(x = x, y = y, col = scale)) + 
  geom_line()

cat("Number of components:\n",
    sapply(res_list, function(res) length(res$fitted_g$pi)), "\n",
    "Number of nonzero components:\n",
    sapply(res_list, function(res) sum(res$fitted_g$pi > 0)))
```

I include two examples with verbose output for inspection. Compare $n = 10000$ with `scale = 1 / 3`:

```{r mixsqp10000}
set.seed(666)
n <- 10000
samp <- samp_from_g(true_g, n) + rnorm(n)
g10000 <- ebnm::ebnm_npmle(samp, scale = 1/3, control = list(verbose = TRUE))
```

and $n = 100000$ with `scale = 1 / 4`:

```{r mixsqp100000}
set.seed(666)
n <- 100000
samp <- samp_from_g(true_g, n) + rnorm(n)
g100000 <- ebnm::ebnm_npmle(samp, scale = 1/4, control = list(verbose = TRUE))
```

