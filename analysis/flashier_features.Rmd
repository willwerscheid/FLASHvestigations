---
title: "Flashier features"
author: "Jason Willwerscheid"
date: "1/12/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


* Handles sparse matrices (of class `Matrix`) and tensors (3-dimensional arrays). 
* Simplifies the user interface. Everything is done via a single function with a small number of parameters, and the latter are more intuitive. In particular, a new `prior.type` parameter replaces the less friendly `ebnm.fn` and `ebnm.param`.
* In constrast, the “workhorse” function gives many more options. One that I especially like allows the user to write an arbitrary function whose output will be displayed during optimization (allowing the user to inspect the progress of optimization however they like).
* Implements a full range of variance structures, including “kronecker” and “noisy.” In general, the estimated residual variance can be an arbitrary rank-one matrix or tensor.
* For simple variance structures (including “constant” and “by row”/“by column”), no $n \times p$ matrix is ever formed (so, for example, a matrix of residuals is never explicitly formed). This yields a large improvement in memory usage and runtime for very large data matrices. (Benchmarking results are [here](flashier_bench.html).)
* Uses a home-grown initialization function rather than `softImpute`. The new function is much faster than `softImpute` for large matrices and deals with fixed elements in a very natural manner.
* Includes new options for speeding up backfits. The “dropout” option drops individual factors once they are no longer improving the objective very much (so, instead of updating every factor each iteration, only factors that are still changing are updated). The “montaigne” option takes this a step further and goes after the factor that most recently produced the largest improvement. This produces much rougher fits, but can greatly reduce the number of backfit iterations.
* Instead of sampling the full $LF'$ matrix, the sampler now just samples $L$ and $F$ separately. This reduces memory usage by a factor of $\min(n, p)$. (With large data matrices, the `flashr` sampler is basically useless because every sample takes up as much memory as the data matrix itself.)
* Includes a new `nonmissing.thresh` parameter to better deal with missing data. See [here](brain.html) for an example.
